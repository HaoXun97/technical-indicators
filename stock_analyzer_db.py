"""
è‚¡ç¥¨æŠ€è¡“åˆ†æèˆ‡è³‡æ–™åº«æ•´åˆç³»çµ±
çµåˆè‚¡ç¥¨æ•¸æ“šæŠ“å–ã€æŠ€è¡“æŒ‡æ¨™è¨ˆç®—å’Œ SQL Server è³‡æ–™åº«å„²å­˜åŠŸèƒ½
"""

import yfinance as yf
import pandas as pd
import numpy as np
import talib
from talib._ta_lib import MA_Type
import configparser
import logging
import time
import sys
from datetime import datetime
from typing import Dict, Optional, List, Any, Union
from dataclasses import dataclass
from enum import Enum
from sqlalchemy import create_engine, text
from contextlib import contextmanager
import warnings

# æŠ‘åˆ¶è­¦å‘Š
warnings.filterwarnings("ignore")


class TimeInterval(Enum):
    """æ™‚é–“é–“éš”æšèˆ‰"""
    MINUTE_1 = "1m"
    MINUTE_5 = "5m"
    MINUTE_15 = "15m"
    MINUTE_30 = "30m"
    HOUR_1 = "1h"
    DAY_1 = "1d"
    WEEK_1 = "1wk"
    MONTH_1 = "1mo"


class Period(Enum):
    """æ™‚é–“é€±æœŸæšèˆ‰"""
    DAY_1 = "1d"
    DAY_5 = "5d"
    MONTH_1 = "1mo"
    MONTH_3 = "3mo"
    MONTH_6 = "6mo"
    YEAR_1 = "1y"
    YEAR_2 = "2y"
    YEAR_5 = "5y"
    YEAR_10 = "10y"
    MAX = "max"


@dataclass
class ProcessResult:
    """è™•ç†çµæœçµ±è¨ˆ"""
    symbol: str
    success: bool
    new_records: int = 0
    updated_records: int = 0
    total_records: int = 0
    error_message: Optional[str] = None
    processing_time: float = 0.0
    date_range: Optional[str] = None


class DatabaseConfig:
    """è³‡æ–™åº«é…ç½®é¡"""

    def __init__(self, config_file: str = "config.ini"):
        self.config = configparser.ConfigParser()
        self.config.read(config_file, encoding='utf-8')

        self.server = self.config.get('database', 'server')
        self.database = self.config.get('database', 'database')
        self.driver = self.config.get('database', 'driver')

        try:
            self.username = self.config.get('database', 'username')
            self.password = self.config.get('database', 'password')
        except (configparser.NoOptionError, configparser.NoSectionError):
            self.username = None
            self.password = None

    def get_sqlalchemy_url(self) -> str:
        """ç”Ÿæˆ SQLAlchemy é€£æ¥å­—ä¸²"""
        if self.username and self.password:
            return (f"mssql+pyodbc://{self.username}:{self.password}@"
                    f"{self.server}/{self.database}?driver={self.driver}")
        else:
            return (f"mssql+pyodbc://@{self.server}/{self.database}"
                    f"?driver={self.driver}&trusted_connection=yes")


class ProgressReporter:
    """é€²åº¦å ±å‘Šå™¨"""

    def __init__(self, logger: logging.Logger):
        self.logger = logger

    def info(self, message: str):
        self.logger.info(message)
        print(f"â„¹ï¸  {message}")

    def success(self, message: str):
        self.logger.info(f"SUCCESS: {message}")
        print(f"âœ… {message}")

    def warning(self, message: str):
        self.logger.warning(message)
        print(f"âš ï¸  {message}")

    def error(self, message: str):
        self.logger.error(message)
        print(f"âŒ {message}")

    def progress(self, message: str):
        self.logger.debug(message)
        print(f"ğŸ”„ {message}")


class StockDataProvider:
    """è‚¡ç¥¨æ•¸æ“šæä¾›è€…"""

    def __init__(self):
        self.logger = logging.getLogger(__name__)
        self._cache = {}

    def get_stock_data(self, symbol: str, period: Union[Period, str, int],
                       interval: Union[TimeInterval, str]
                       ) -> Optional[pd.DataFrame]:
        """ç²å–è‚¡ç¥¨æ•¸æ“š - æ”¯æ´å‹•æ…‹æœŸé–“åƒæ•¸"""
        try:
            formatted_symbol = self._format_symbol(symbol)

            # è™•ç†ä¸åŒé¡å‹çš„ period åƒæ•¸
            if isinstance(period, Period):
                period_str = period.value
            elif isinstance(period, int):
                period_str = f"{period}d"  # æ•´æ•¸å¤©æ•¸è½‰æ›ç‚ºå¤©æ•¸å­—ä¸²
            else:
                period_str = period

            interval_str = interval.value if isinstance(
                interval, TimeInterval) else interval

            # èª¿æ•´æœŸé–“ä»¥é¿å…APIé™åˆ¶
            adjusted_period = self._adjust_period_for_interval(
                period_str, interval_str)

            # å˜—è©¦ç²å–æ•¸æ“š
            symbols_to_try = [formatted_symbol]
            if (formatted_symbol.endswith('.TW') and symbol.isdigit()
                    and len(symbol) == 4):
                symbols_to_try.append(f"{symbol}.TWO")

            for attempt_symbol in symbols_to_try:
                # æŠ‘åˆ¶ yfinance è¼¸å‡º
                import io
                from contextlib import redirect_stderr, redirect_stdout

                loggers_to_silence = ['yfinance', 'urllib3', 'requests']
                original_levels = {}
                for logger_name in loggers_to_silence:
                    logger = logging.getLogger(logger_name)
                    original_levels[logger_name] = logger.level
                    logger.setLevel(logging.CRITICAL)

                devnull = io.StringIO()

                try:
                    with redirect_stderr(devnull), redirect_stdout(devnull):
                        ticker = yf.Ticker(attempt_symbol)
                        data = ticker.history(
                            period=adjusted_period,
                            interval=interval_str,
                            auto_adjust=False,
                            actions=False,
                            timeout=10
                        )

                        if not data.empty:
                            # ç§»é™¤ Adj Close æ¬„ä½
                            if 'Adj Close' in data.columns:
                                data = data.drop(columns=['Adj Close'])

                            self.logger.info(
                                f"æˆåŠŸç²å– {attempt_symbol} æ•¸æ“šï¼š{len(data)} ç­†")
                            return data

                finally:
                    # æ¢å¾©æ—¥èªŒç´šåˆ¥
                    for logger_name, level in original_levels.items():
                        logging.getLogger(logger_name).setLevel(level)

            self.logger.error(f"ç„¡æ³•ç²å– {symbol} çš„æ•¸æ“š")
            return None

        except Exception as e:
            self.logger.error(f"ç²å– {symbol} æ•¸æ“šéŒ¯èª¤: {e}")
            return None

    def _format_symbol(self, symbol: str) -> str:
        """æ ¼å¼åŒ–è‚¡ç¥¨ä»£è™Ÿ"""
        if symbol.isdigit() and len(symbol) == 4:
            return f"{symbol}.TW"

        if symbol.endswith((".TW", ".TWO")):
            return symbol

        if (any(c.isalpha() for c in symbol) and
                len(symbol) <= 5 and not symbol.isdigit()):
            return symbol

        return f"{symbol}.TW"

    def _adjust_period_for_interval(self, period: str, interval: str) -> str:
        """æ ¹æ“šé–“éš”èª¿æ•´æœŸé–“"""
        if interval in ["1h", "2h", "4h", "6h", "12h"]:
            if period in ["max", "5y", "10y"]:
                return "730d"
        elif interval in ["1m", "2m", "5m", "15m", "30m", "90m"]:
            if period in ["max", "5y", "10y", "2y", "1y"]:
                return "60d"
        return period


class TechnicalIndicatorCalculator:
    """æŠ€è¡“æŒ‡æ¨™è¨ˆç®—å™¨"""

    def __init__(self):
        self.logger = logging.getLogger(__name__)

    def calculate_all_indicators(self, data: pd.DataFrame) -> pd.DataFrame:
        """è¨ˆç®—æ‰€æœ‰æŠ€è¡“æŒ‡æ¨™ä¸¦è¿”å›å®Œæ•´çš„ DataFrame"""
        if data.empty or len(data) < 60:
            raise ValueError("æ•¸æ“šä¸è¶³ï¼Œç„¡æ³•è¨ˆç®—æŠ€è¡“æŒ‡æ¨™")

        # å‰µå»ºçµæœ DataFrame
        result_df = data.copy()

        # è½‰æ›ç‚º numpy arrays
        high = data["High"].values.astype("float64")
        low = data["Low"].values.astype("float64")
        close = data["Close"].values.astype("float64")
        # volume = data["Volume"].values.astype("float64")

        try:
            # RSI æŒ‡æ¨™
            result_df['RSI(5)'] = talib.RSI(close, timeperiod=5)
            result_df['RSI(7)'] = talib.RSI(close, timeperiod=7)
            result_df['RSI(10)'] = talib.RSI(close, timeperiod=10)
            result_df['RSI(14)'] = talib.RSI(close, timeperiod=14)
            result_df['RSI(21)'] = talib.RSI(close, timeperiod=21)

            # MACD æŒ‡æ¨™
            macd_line, signal_line, macd_histogram = talib.MACD(
                close, fastperiod=12, slowperiod=26, signalperiod=9)
            result_df['DIF'] = macd_line
            result_df['MACD'] = signal_line
            result_df['MACD_Histogram'] = macd_histogram

            # KDJ æŒ‡æ¨™
            kdj_data = self._calculate_kdj(high, low, close)
            result_df['RSV'] = kdj_data['RSV']
            result_df['K'] = kdj_data['K']
            result_df['D'] = kdj_data['D']
            result_df['J'] = kdj_data['J']

            # ç§»å‹•å¹³å‡ç·š
            result_df['MA5'] = talib.SMA(close, timeperiod=5)
            result_df['MA10'] = talib.SMA(close, timeperiod=10)
            result_df['MA20'] = talib.SMA(close, timeperiod=20)
            result_df['MA60'] = talib.SMA(close, timeperiod=60)
            result_df['EMA12'] = talib.EMA(close, timeperiod=12)
            result_df['EMA26'] = talib.EMA(close, timeperiod=26)

            # å¸ƒæ—é€šé“
            bb_upper, bb_middle, bb_lower = talib.BBANDS(
                close, timeperiod=20, nbdevup=2, nbdevdn=2, matype=MA_Type.SMA)
            result_df['BB_Upper'] = bb_upper
            result_df['BB_Middle'] = bb_middle
            result_df['BB_Lower'] = bb_lower

            # å…¶ä»–æŒ‡æ¨™
            result_df['ATR'] = talib.ATR(high, low, close, timeperiod=14)
            result_df['CCI'] = talib.CCI(high, low, close, timeperiod=14)
            result_df['WILLR'] = talib.WILLR(high, low, close, timeperiod=20)
            result_df['MOM'] = talib.MOM(close, timeperiod=10)

            self.logger.info("æŠ€è¡“æŒ‡æ¨™è¨ˆç®—å®Œæˆ")
            return result_df

        except Exception as e:
            self.logger.error(f"è¨ˆç®—æŠ€è¡“æŒ‡æ¨™éŒ¯èª¤: {e}")
            raise

    def _calculate_kdj(self, high: np.ndarray, low: np.ndarray,
                       close: np.ndarray) -> Dict[str, np.ndarray]:
        """è¨ˆç®— KDJ æŒ‡æ¨™"""
        n = 9
        rsv = np.full(len(close), np.nan)
        k_values = np.full(len(close), np.nan)
        d_values = np.full(len(close), np.nan)

        # è¨ˆç®— RSV
        for i in range(n - 1, len(close)):
            period_high = np.max(high[i - n + 1: i + 1])
            period_low = np.min(low[i - n + 1: i + 1])

            if period_high != period_low:
                rsv_value = ((close[i] - period_low) /
                             (period_high - period_low)) * 100
                rsv[i] = max(0, min(100, rsv_value))
            else:
                rsv[i] = 50

        # è¨ˆç®— K å’Œ D
        first_valid_idx = n - 1
        k_values[first_valid_idx] = rsv[first_valid_idx] if not np.isnan(
            rsv[first_valid_idx]) else 50
        d_values[first_valid_idx] = k_values[first_valid_idx]

        for i in range(first_valid_idx + 1, len(close)):
            if not np.isnan(rsv[i]):
                k_values[i] = (2 / 3) * k_values[i - 1] + (1 / 3) * rsv[i]
                d_values[i] = (2 / 3) * d_values[i - 1] + (1 / 3) * k_values[i]

        # è¨ˆç®— J
        j_values = 3 * k_values - 2 * d_values

        return {
            'RSV': rsv,
            'K': k_values,
            'D': d_values,
            'J': j_values
        }


class StockAnalyzerDB:
    """è‚¡ç¥¨åˆ†æè³‡æ–™åº«æ•´åˆç³»çµ±"""

    def __init__(self, config_file: str = "config.ini"):
        # åˆå§‹åŒ–é…ç½®
        self.config = configparser.ConfigParser()
        self.config.read(config_file, encoding='utf-8')

        self.db_config = DatabaseConfig(config_file)
        self.data_provider = StockDataProvider()
        self.indicator_calculator = TechnicalIndicatorCalculator()

        # è¨­ç½®æ—¥èªŒ
        log_level = self.config.get(
            'import_settings', 'log_level', fallback='INFO')
        self.logger = self._setup_logger(log_level)
        self.reporter = ProgressReporter(self.logger)

        # åˆå§‹åŒ–è³‡æ–™åº«é€£æ¥
        self.engine = create_engine(
            self.db_config.get_sqlalchemy_url(),
            pool_pre_ping=True,
            pool_recycle=3600
        )

        # ç¢ºä¿è³‡æ–™è¡¨å­˜åœ¨
        self._ensure_table_exists()

    def _setup_logger(self, log_level: str = 'INFO') -> logging.Logger:
        """è¨­ç½®æ—¥èªŒè¨˜éŒ„å™¨"""
        logger = logging.getLogger('StockAnalyzerDB')
        logger.handlers = []

        level = getattr(logging, log_level.upper(), logging.INFO)
        logger.setLevel(level)

        try:
            file_handler = logging.FileHandler(
                'stock_analyzer.log', encoding='utf-8')
            formatter = logging.Formatter(
                '%(asctime)s - %(name)s - %(levelname)s - %(message)s')
            file_handler.setFormatter(formatter)
            logger.addHandler(file_handler)
        except Exception:
            pass

        return logger

    @contextmanager
    def get_connection(self):
        """è³‡æ–™åº«é€£æ¥ä¸Šä¸‹æ–‡ç®¡ç†å™¨"""
        conn = self.engine.connect()
        try:
            yield conn
        finally:
            conn.close()

    def test_connection(self) -> bool:
        """æ¸¬è©¦è³‡æ–™åº«é€£æ¥"""
        try:
            with self.get_connection() as conn:
                conn.execute(text("SELECT 1"))
            self.reporter.success("è³‡æ–™åº«é€£æ¥æ¸¬è©¦æˆåŠŸ")
            return True
        except Exception as e:
            self.reporter.error(f"è³‡æ–™åº«é€£æ¥æ¸¬è©¦å¤±æ•—: {e}")
            return False

    def _ensure_table_exists(self):
        """ç¢ºä¿è³‡æ–™è¡¨å­˜åœ¨"""
        create_table_sql = text("""
        IF NOT EXISTS (SELECT * FROM sysobjects
                                WHERE name='stock_data' AND xtype='U')
        BEGIN
            CREATE TABLE stock_data (
                id BIGINT IDENTITY(1,1) PRIMARY KEY,
                symbol NVARCHAR(10) NOT NULL,
                date DATE NOT NULL,
                open_price DECIMAL(18,6),
                high_price DECIMAL(18,6),
                low_price DECIMAL(18,6),
                close_price DECIMAL(18,6),
                volume BIGINT,
                rsi_5 DECIMAL(8,4),
                rsi_7 DECIMAL(8,4),
                rsi_10 DECIMAL(8,4),
                rsi_14 DECIMAL(8,4),
                rsi_21 DECIMAL(8,4),
                dif DECIMAL(10,6),
                macd DECIMAL(10,6),
                macd_histogram DECIMAL(10,6),
                rsv DECIMAL(8,4),
                k_value DECIMAL(8,4),
                d_value DECIMAL(8,4),
                j_value DECIMAL(8,4),
                ma5 DECIMAL(18,6),
                ma10 DECIMAL(18,6),
                ma20 DECIMAL(18,6),
                ma60 DECIMAL(18,6),
                ema12 DECIMAL(18,6),
                ema26 DECIMAL(18,6),
                bb_upper DECIMAL(18,6),
                bb_middle DECIMAL(18,6),
                bb_lower DECIMAL(18,6),
                atr DECIMAL(10,6),
                cci DECIMAL(10,4),
                willr DECIMAL(8,4),
                mom DECIMAL(10,6),
                created_at DATETIME2 DEFAULT GETDATE(),
                updated_at DATETIME2 DEFAULT GETDATE(),
                CONSTRAINT UK_stock_symbol_date UNIQUE (symbol, date)
            );

            -- å‰µå»ºç´¢å¼•
            CREATE NONCLUSTERED INDEX IX_stock_data_symbol_date
                ON stock_data (symbol, date DESC);
            CREATE NONCLUSTERED INDEX IX_stock_data_date
                ON stock_data (date DESC);
            CREATE NONCLUSTERED INDEX IX_stock_data_symbol
                ON stock_data (symbol);
        END
        """)

        try:
            with self.get_connection() as conn:
                conn.execute(create_table_sql)
                conn.commit()
            self.logger.info("è³‡æ–™è¡¨æª¢æŸ¥/å‰µå»ºå®Œæˆ")
        except Exception as e:
            self.reporter.error(f"å‰µå»ºè³‡æ–™è¡¨å¤±æ•—: {e}")
            raise

    def get_existing_data_info(self, symbol: str) -> Dict[str, Any]:
        """ç²å–å·²å­˜åœ¨çš„æ•¸æ“šè³‡è¨Š"""
        try:
            with self.get_connection() as conn:
                query = text("""
                SELECT
                    COUNT(*) as record_count,
                    MIN(date) as earliest_date,
                    MAX(date) as latest_date,
                    MAX(updated_at) as last_updated
                FROM stock_data
                WHERE symbol = :symbol
                """)

                result = conn.execute(query, {"symbol": symbol}).fetchone()

                if result and result[0] > 0:
                    return {
                        'exists': True,
                        'record_count': result[0],
                        'earliest_date': result[1],
                        'latest_date': result[2],
                        'last_updated': result[3]
                    }
                else:
                    return {'exists': False}

        except Exception as e:
            self.logger.warning(f"ç²å–æ•¸æ“šè³‡è¨Šå¤±æ•—: {e}")
            return {'exists': False}

    def process_stock(self, symbol: str,
                      period: Union[Period, str] = Period.MAX,
                      interval: Union[TimeInterval, str] = TimeInterval.DAY_1,
                      force_update: bool = False
                      ) -> ProcessResult:
        """è™•ç†å–®ä¸€è‚¡ç¥¨ï¼šç²å–æ•¸æ“šã€è¨ˆç®—æŒ‡æ¨™ã€å„²å­˜åˆ°è³‡æ–™åº«

        Args:
            symbol: è‚¡ç¥¨ä»£è™Ÿ
            period: æ™‚é–“é€±æœŸ
            interval: æ™‚é–“é–“éš”
            force_update: æ˜¯å¦å¼·åˆ¶æ›´æ–°æ‰€æœ‰æ•¸æ“šï¼ˆåŒ…æ‹¬å·²å­˜åœ¨çš„æ•¸æ“šï¼‰
        """
        start_time = time.time()
        result = ProcessResult(symbol=symbol, success=False)
        new_data = pd.DataFrame()  # åˆå§‹åŒ– new_data è®Šæ•¸

        try:
            self.reporter.progress(f"é–‹å§‹è™•ç†è‚¡ç¥¨ {symbol}")

            # 1. é¦–å…ˆæª¢æŸ¥è³‡æ–™åº«ä¸­çš„ç¾æœ‰æ•¸æ“š
            existing_info = self.get_existing_data_info(symbol)

            # 2. ç²å–å®Œæ•´çš„è‚¡ç¥¨æ•¸æ“šï¼ˆæ ¹æ“šæŒ‡å®šçš„æœŸé–“ï¼‰
            fetch_period = period.value if isinstance(
                period, Period) else period

            data = self.data_provider.get_stock_data(
                symbol, fetch_period, interval)
            if data is None or data.empty:
                result.error_message = "ç„¡æ³•ç²å–è‚¡ç¥¨æ•¸æ“š"
                return result

            self.reporter.info(f"{symbol}: ç²å–åˆ° {len(data)} ç­†åŸå§‹æ•¸æ“š")

            # 3. è¨ˆç®—æŠ€è¡“æŒ‡æ¨™
            try:
                complete_data = (
                    self.indicator_calculator.calculate_all_indicators(data)
                )
                self.reporter.info(f"{symbol}: æŠ€è¡“æŒ‡æ¨™è¨ˆç®—å®Œæˆ")
            except Exception as e:
                result.error_message = f"æŠ€è¡“æŒ‡æ¨™è¨ˆç®—å¤±æ•—: {e}"
                return result

            # 4. åˆ¤æ–·éœ€è¦å„²å­˜çš„æ•¸æ“šç¯„åœ
            if force_update:
                # å¼·åˆ¶æ›´æ–°æ¨¡å¼ï¼šé‡æ–°è™•ç†æ‰€æœ‰æ•¸æ“š
                self.reporter.info(f"{symbol}: å¼·åˆ¶æ›´æ–°æ¨¡å¼ - å°‡é‡æ–°è™•ç†æ‰€æœ‰æ•¸æ“š")
                new_data = complete_data
                saved_count = self._save_data_to_db(
                    complete_data, symbol, mode='replace')
                result.new_records = saved_count
                self.reporter.success(
                    f"{symbol}: å¼·åˆ¶æ›´æ–°å®Œæˆ - è™•ç†äº† {saved_count} ç­†æ•¸æ“š")

            elif existing_info['exists']:
                earliest_db_date = existing_info['earliest_date']
                latest_db_date = existing_info['latest_date']

                # çµ±ä¸€è™•ç†æ—¥æœŸæ ¼å¼
                if hasattr(earliest_db_date, 'date'):
                    earliest_db_date = earliest_db_date.date()
                elif isinstance(earliest_db_date, str):
                    from datetime import datetime
                    earliest_db_date = datetime.strptime(
                        earliest_db_date, '%Y-%m-%d').date()

                if hasattr(latest_db_date, 'date'):
                    latest_db_date = latest_db_date.date()
                elif isinstance(latest_db_date, str):
                    from datetime import datetime
                    latest_db_date = datetime.strptime(
                        latest_db_date, '%Y-%m-%d').date()

                # åˆ†ææ–°ç²å–çš„æ•¸æ“šç¯„åœ
                data_dates = complete_data.index.to_series().dt.date
                earliest_data_date = data_dates.min()
                latest_data_date = data_dates.max()

                # æ‰¾å‡ºéœ€è¦æ–°å¢çš„æ•¸æ“šï¼ˆåŒ…æ‹¬æ›´æ—©å’Œæ›´æ–°çš„æ•¸æ“šï¼‰
                # æ›´æ—©çš„æ­·å²æ•¸æ“š
                historical_data = complete_data[data_dates < earliest_db_date]
                future_data = complete_data[data_dates >
                                            latest_db_date]        # æ›´æ–°çš„æ•¸æ“š

                new_data = pd.concat([historical_data, future_data])

                if not new_data.empty:
                    saved_count = self._save_data_to_db(
                        new_data, symbol, mode='append')
                    result.new_records = saved_count

                    # æä¾›è©³ç´°çš„æ›´æ–°è³‡è¨Š
                    historical_count = len(historical_data)
                    future_count = len(future_data)

                    update_msg = f"{symbol}: æ–°å¢ {saved_count} ç­†æ•¸æ“š"
                    if historical_count > 0 and future_count > 0:
                        update_msg += f" (æ­·å²: {historical_count} ç­†, æœ€æ–°: "
                        f"{future_count} ç­†)"
                    elif historical_count > 0:
                        update_msg += f" (æ­·å²æ•¸æ“š: {historical_count} ç­†)"
                    elif future_count > 0:
                        update_msg += f" (æœ€æ–°æ•¸æ“š: {future_count} ç­†)"

                    self.reporter.success(update_msg)

                    # é¡¯ç¤ºæ•¸æ“šè¦†è“‹ç¯„åœ
                    if historical_count > 0:
                        hist_start = historical_data.index.min().strftime(
                            '%Y-%m-%d')
                        hist_end = historical_data.index.max().strftime(
                            '%Y-%m-%d')
                        self.reporter.info(
                            f"{symbol}: æ–°å¢æ­·å²æ•¸æ“šç¯„åœ: {hist_start} ~ {hist_end}")

                    if future_count > 0:
                        fut_start = future_data.index.min().strftime(
                            '%Y-%m-%d')
                        fut_end = future_data.index.max().strftime(
                            '%Y-%m-%d')
                        self.reporter.info(
                            f"{symbol}: æ–°å¢æœ€æ–°æ•¸æ“šç¯„åœ: {fut_start} ~ {fut_end}")

                else:
                    # æª¢æŸ¥æ˜¯å¦éœ€è¦æç¤ºä½¿ç”¨å¼·åˆ¶æ›´æ–°
                    if (earliest_data_date <= latest_db_date and
                            latest_data_date >= earliest_db_date):
                        self.reporter.info(f"{symbol}: æ•¸æ“šåº«å·²åŒ…å«æ‰€æœ‰å¯ç”¨æ•¸æ“šï¼Œç„¡éœ€æ›´æ–°")
                        self.reporter.info(
                            f"{symbol}: å¦‚éœ€æ›´æ–°å·²å­˜åœ¨çš„æ•¸æ“šï¼Œè«‹ä½¿ç”¨ force_update=True åƒæ•¸")
                    else:
                        self.reporter.info(f"{symbol}: æ•¸æ“šåº«å·²åŒ…å«æ‰€æœ‰å¯ç”¨æ•¸æ“šï¼Œç„¡éœ€æ›´æ–°")

                    result.success = True
                    result.total_records = existing_info['record_count']
                    result.processing_time = time.time() - start_time
                    return result
            else:
                # å…¨æ–°è‚¡ç¥¨ï¼Œå„²å­˜æ‰€æœ‰æ•¸æ“š
                new_data = complete_data  # ç¢ºä¿ new_data è¢«å®šç¾©
                saved_count = self._save_data_to_db(
                    complete_data, symbol, mode='replace')
                result.new_records = saved_count
                self.reporter.success(f"{symbol}: å„²å­˜ {saved_count} ç­†å®Œæ•´æ•¸æ“š")

            # 5. æ›´æ–°çµæœçµ±è¨ˆ
            result.success = True
            result.total_records = len(complete_data)
            result.processing_time = time.time() - start_time

            # è¨­ç½®æ—¥æœŸç¯„åœ
            if not new_data.empty:
                start_date = new_data.index.min().strftime('%Y-%m-%d')
                end_date = new_data.index.max().strftime('%Y-%m-%d')
                result.date_range = f"{start_date} ~ {end_date}"
            else:
                start_date = complete_data.index.min().strftime('%Y-%m-%d')
                end_date = complete_data.index.max().strftime('%Y-%m-%d')
                result.date_range = f"{start_date} ~ {end_date}"

        except Exception as e:
            result.error_message = str(e)
            result.processing_time = time.time() - start_time
            self.reporter.error(f"{symbol}: è™•ç†å¤±æ•— - {e}")

        return result

    def _save_data_to_db(self, data: pd.DataFrame,
                         symbol: str, mode: str = 'append') -> int:
        """å„²å­˜æ•¸æ“šåˆ°è³‡æ–™åº«"""
        if data.empty:
            self.logger.warning(f"{symbol}: è¦å„²å­˜çš„æ•¸æ“šç‚ºç©º")
            return 0

        self.logger.info(f"{symbol}: æº–å‚™å„²å­˜ {len(data)} ç­†æ•¸æ“šï¼Œæ¨¡å¼: {mode}")

        # æº–å‚™è³‡æ–™åº«æ ¼å¼çš„æ•¸æ“š
        db_data = self._prepare_data_for_db(data, symbol)

        if db_data.empty:
            self.logger.error(f"{symbol}: æº–å‚™è³‡æ–™åº«æ ¼å¼çš„æ•¸æ“šå¤±æ•—ï¼Œçµæœç‚ºç©º")
            return 0

        self.logger.info(f"{symbol}: æº–å‚™äº† {len(db_data)} ç­†è³‡æ–™åº«æ ¼å¼æ•¸æ“š")

        try:
            if mode == 'replace':
                # å…ˆåˆªé™¤ç¾æœ‰æ•¸æ“š
                with self.get_connection() as conn:
                    delete_sql = text(
                        "DELETE FROM stock_data WHERE symbol = :symbol")
                    delete_result = conn.execute(
                        delete_sql, {"symbol": symbol})
                    deleted_rows = delete_result.rowcount
                    conn.commit()
                    self.logger.info(f"{symbol}: åˆªé™¤äº† {deleted_rows} ç­†èˆŠæ•¸æ“š")

            # ä½¿ç”¨ UPSERT æ–¹å¼å„²å­˜
            saved_count = self._upsert_data(db_data)
            self.logger.info(f"{symbol}: æˆåŠŸå„²å­˜ {saved_count} ç­†æ•¸æ“š")
            return saved_count

        except Exception as e:
            self.logger.error(f"{symbol}: å„²å­˜æ•¸æ“šå¤±æ•—: {e}")
            # æ‰“å°è©³ç´°çš„éŒ¯èª¤è³‡è¨Š
            import traceback
            self.logger.error(f"{symbol}: è©³ç´°éŒ¯èª¤: {traceback.format_exc()}")
            raise

    def _prepare_data_for_db(self, data: pd.DataFrame,
                             symbol: str) -> pd.DataFrame:
        """æº–å‚™è³‡æ–™åº«æ ¼å¼çš„æ•¸æ“š"""
        try:
            self.logger.info(f"{symbol}: é–‹å§‹æº–å‚™è³‡æ–™åº«æ ¼å¼æ•¸æ“šï¼ŒåŸå§‹æ•¸æ“šå½¢ç‹€: {data.shape}")
            self.logger.info(f"{symbol}: åŸå§‹æ•¸æ“šæ¬„ä½: {list(data.columns)}")

            # å…ˆç²å–è¡Œæ•¸ï¼Œç„¶å¾Œå‰µå»ºåŒ…å«æ‰€æœ‰å¿…è¦æ¬„ä½çš„ DataFrame
            num_rows = len(data)

            # å®‰å…¨åœ°è™•ç†æ—¥æœŸç´¢å¼• - å®Œå…¨é¿å… .date å±¬æ€§
            try:
                # ç¢ºä¿ç´¢å¼•æ˜¯ datetime é¡å‹
                if isinstance(data.index, pd.DatetimeIndex):
                    datetime_index = data.index
                else:
                    datetime_index = pd.to_datetime(data.index)

                # ä½¿ç”¨ normalize() æ–¹æ³•å»é™¤æ™‚é–“éƒ¨åˆ†ï¼Œåªä¿ç•™æ—¥æœŸ
                dates = datetime_index.normalize().date

            except (AttributeError, TypeError):
                # å¦‚æœé‚„æ˜¯å¤±æ•—ï¼Œä½¿ç”¨æ›´å®‰å…¨çš„æ–¹æ³•
                try:
                    datetime_index = pd.to_datetime(data.index)
                    # è½‰æ›ç‚ºæ—¥æœŸå­—ç¬¦ä¸²ï¼Œç„¶å¾Œå†è½‰å›æ—¥æœŸå°è±¡
                    date_strings = datetime_index.strftime('%Y-%m-%d')
                    dates = [datetime.strptime(ds, '%Y-%m-%d').date()
                             for ds in date_strings]
                except Exception as e:
                    self.logger.error(f"{symbol}: æ—¥æœŸè½‰æ›å¤±æ•—: {e}")
                    # æœ€å¾Œå‚™ç”¨æ–¹æ¡ˆï¼šä½¿ç”¨å­—ç¬¦ä¸²æ ¼å¼
                    dates = pd.to_datetime(data.index).strftime('%Y-%m-%d')

            # åŸºæœ¬è³‡æ–™ - æ­£ç¢ºå‰µå»º DataFrame
            db_data = pd.DataFrame({
                'symbol': [symbol] * num_rows,  # å‰µå»ºèˆ‡æ•¸æ“šè¡Œæ•¸ç›¸åŒçš„ symbol æ¬„ä½
                'date': dates,
                'open_price': data['Open'].where(
                    pd.notnull(data['Open']), None),
                'high_price': data['High'].where(
                    pd.notnull(data['High']), None),
                'low_price': data['Low'].where(
                    pd.notnull(data['Low']), None),
                'close_price': data['Close'].where(
                    pd.notnull(data['Close']), None),
                'volume': data['Volume'].where(
                    pd.notnull(data['Volume']), None).astype('Int64')
            })

            # æŠ€è¡“æŒ‡æ¨™æ˜ å°„
            indicator_mapping = {
                'RSI(5)': 'rsi_5',
                'RSI(7)': 'rsi_7',
                'RSI(10)': 'rsi_10',
                'RSI(14)': 'rsi_14',
                'RSI(21)': 'rsi_21',
                'DIF': 'dif',
                'MACD': 'macd',
                'MACD_Histogram': 'macd_histogram',
                'RSV': 'rsv',
                'K': 'k_value',
                'D': 'd_value',
                'J': 'j_value',
                'MA5': 'ma5',
                'MA10': 'ma10',
                'MA20': 'ma20',
                'MA60': 'ma60',
                'EMA12': 'ema12',
                'EMA26': 'ema26',
                'BB_Upper': 'bb_upper',
                'BB_Middle': 'bb_middle',
                'BB_Lower': 'bb_lower',
                'ATR': 'atr',
                'CCI': 'cci',
                'WILLR': 'willr',
                'MOM': 'mom'
            }

            # æª¢æŸ¥ä¸¦æ·»åŠ æŠ€è¡“æŒ‡æ¨™ï¼Œæ­£ç¢ºè™•ç† NaN å€¼
            indicators_found = []
            for file_col, db_col in indicator_mapping.items():
                if file_col in data.columns:
                    indicators_found.append(file_col)
                    # ä½¿ç”¨ where æ–¹æ³•ä¿æŒ NaN ç‚º Noneï¼Œä¸¦æ‡‰ç”¨é©ç•¶çš„å››æ¨äº”å…¥
                    if db_col in ['rsi_5', 'rsi_7', 'rsi_10', 'rsi_14',
                                  'rsi_21', 'rsv', 'k_value', 'd_value',
                                  'j_value', 'willr']:
                        # å°æ–¼ç™¾åˆ†æ¯”æŒ‡æ¨™ï¼ŒNaN ä¿æŒç‚º None
                        series_data = data[file_col].round(4)
                        db_data[db_col] = series_data.where(
                            pd.notnull(series_data), None)
                    elif db_col in ['dif', 'macd', 'macd_histogram',
                                    'atr', 'mom']:
                        series_data = data[file_col].round(6)
                        db_data[db_col] = series_data.where(
                            pd.notnull(series_data), None)
                    elif db_col in ['cci']:
                        series_data = data[file_col].round(4)
                        db_data[db_col] = series_data.where(
                            pd.notnull(series_data), None)
                    else:  # MA å’Œ BB ç³»åˆ—
                        series_data = data[file_col].round(6)
                        db_data[db_col] = series_data.where(
                            pd.notnull(series_data), None)
                else:
                    # å¦‚æœæŠ€è¡“æŒ‡æ¨™æ¬„ä½ä¸å­˜åœ¨ï¼Œè¨­ç‚º None
                    db_data[db_col] = None

            self.logger.info(f"{symbol}: æ‰¾åˆ°æŠ€è¡“æŒ‡æ¨™: {indicators_found}")
            self.logger.info(f"{symbol}: æº–å‚™å¾Œçš„æ•¸æ“šå½¢ç‹€: {db_data.shape}")

            # æª¢æŸ¥æ•¸æ“šå®Œæ•´æ€§
            valid_dates = db_data['date'].notna().sum()
            valid_symbols = db_data['symbol'].notna().sum()
            self.logger.info(f"{symbol}: æœ‰æ•ˆæ—¥æœŸæ•¸é‡: {valid_dates}")
            self.logger.info(f"{symbol}: æœ‰æ•ˆ symbol æ•¸é‡: {valid_symbols}")

            # æª¢æŸ¥ NULL å€¼çš„æƒ…æ³ï¼ˆåªé¡¯ç¤ºæœ‰ NULL çš„æ¬„ä½ï¼‰
            nan_counts = db_data.isnull().sum()
            nan_fields = {col: count for col, count in nan_counts.items(
            ) if count > 0 and count < len(db_data)}
            if nan_fields:
                self.logger.info(f"{symbol}: éƒ¨åˆ† NULL å€¼æ¬„ä½: {nan_fields}")

            # é©—è­‰ symbol æ¬„ä½æ˜¯å¦æ­£ç¢º
            if valid_symbols != num_rows:
                self.logger.error(
                    f"{symbol}: symbol æ¬„ä½è¨­ç½®å¤±æ•—ï¼é æœŸ: "
                    f"{num_rows}, å¯¦éš›: {valid_symbols}")
                # å˜—è©¦ä¿®å¾©
                db_data['symbol'] = symbol
                valid_symbols_after_fix = db_data['symbol'].notna().sum()
                self.logger.info(
                    f"{symbol}: ä¿®å¾©å¾Œ symbol æ•¸é‡: {valid_symbols_after_fix}")

            return db_data

        except Exception as e:
            self.logger.error(f"{symbol}: æº–å‚™è³‡æ–™åº«æ ¼å¼æ•¸æ“šå¤±æ•—: {e}")
            import traceback
            self.logger.error(f"{symbol}: è©³ç´°éŒ¯èª¤: {traceback.format_exc()}")
            return pd.DataFrame()

    def _upsert_data(self, data: pd.DataFrame) -> int:
        """ä½¿ç”¨ UPSERT æ–¹å¼å„²å­˜æ•¸æ“š"""
        if data.empty:
            self.logger.warning("UPSERT: æ•¸æ“šç‚ºç©º")
            return 0

        symbol = data['symbol'].iloc[0] if not data.empty else "UNKNOWN"
        self.logger.info(f"{symbol}: é–‹å§‹ UPSERT æ“ä½œï¼Œæ•¸æ“šç­†æ•¸: {len(data)}")

        merge_sql = text("""
        MERGE stock_data AS target
        USING (SELECT :symbol as symbol, :date as date) AS source
        ON (target.symbol = source.symbol AND target.date = source.date)
        WHEN MATCHED THEN
            UPDATE SET
                open_price = :open_price, high_price = :high_price,
                low_price = :low_price, close_price = :close_price,
                volume = :volume, rsi_5 = :rsi_5, rsi_7 = :rsi_7,
                rsi_10 = :rsi_10, rsi_14 = :rsi_14, rsi_21 = :rsi_21,
                dif = :dif, macd = :macd, macd_histogram = :macd_histogram,
                rsv = :rsv, k_value = :k_value, d_value = :d_value,
                j_value = :j_value, ma5 = :ma5, ma10 = :ma10,
                ma20 = :ma20, ma60 = :ma60, ema12 = :ema12, ema26 = :ema26,
                bb_upper = :bb_upper, bb_middle = :bb_middle,
                bb_lower = :bb_lower, atr = :atr, cci = :cci,
                willr = :willr, mom = :mom, updated_at = GETDATE()
        WHEN NOT MATCHED THEN
            INSERT (symbol, date, open_price, high_price,
                         low_price, close_price, volume,
                         rsi_5, rsi_7, rsi_10, rsi_14, rsi_21, dif, macd,
                   macd_histogram, rsv, k_value, d_value, j_value, ma5, ma10,
                   ma20, ma60, ema12, ema26, bb_upper, bb_middle, bb_lower,
                   atr, cci, willr, mom)
            VALUES (:symbol, :date, :open_price, :high_price, :low_price,
                   :close_price, :volume, :rsi_5, :rsi_7, :rsi_10, :rsi_14,
                   :rsi_21, :dif, :macd, :macd_histogram, :rsv, :k_value,
                   :d_value, :j_value, :ma5, :ma10, :ma20, :ma60, :ema12,
                   :ema26, :bb_upper, :bb_middle, :bb_lower, :atr, :cci,
                   :willr, :mom)
        OUTPUT $action;
        """)

        saved_count = 0
        error_count = 0

        try:
            with self.get_connection() as conn:
                for i, (_, row) in enumerate(data.iterrows()):
                    try:
                        params = row.to_dict()

                        # ç¢ºä¿æ‰€æœ‰åƒæ•¸éƒ½æ˜¯æœ‰æ•ˆçš„é¡å‹
                        for key, value in params.items():
                            if pd.isna(value):
                                params[key] = None
                            elif key == 'volume' and value is not None:
                                params[key] = int(value)

                        # è¨˜éŒ„å‰å¹¾ç­†æ•¸æ“šçš„è©³ç´°è³‡è¨Š
                        if i < 3:
                            self.logger.debug(
                                f"{symbol}: ç¬¬ {i+1} ç­†æ•¸æ“šåƒæ•¸: {params}")

                        result = conn.execute(merge_sql, params)
                        action = result.fetchone()

                        if action and action[0] in ['INSERT', 'UPDATE']:
                            saved_count += 1
                            if i < 3:
                                self.logger.debug(
                                    f"{symbol}: ç¬¬ {i+1} ç­†åŸ·è¡Œå‹•ä½œ: {action[0]}")
                        else:
                            if i < 3:
                                self.logger.warning(
                                    f"{symbol}: ç¬¬ {i+1} ç­†ç„¡å‹•ä½œå›å‚³: {action}")

                    except Exception as e:
                        error_count += 1
                        if error_count <= 5:  # åªè¨˜éŒ„å‰5å€‹éŒ¯èª¤
                            self.logger.error(
                                f"{symbol}: ç¬¬ {i+1} ç­†æ•¸æ“šå„²å­˜å¤±æ•—: {e}")
                            if i < 3:  # åªåœ¨å‰3ç­†æ™‚è¨˜éŒ„è©³ç´°æ•¸æ“š
                                self.logger.debug(
                                    f"{symbol}: å¤±æ•—çš„æ•¸æ“š: {row.to_dict()}")

                conn.commit()
                self.logger.info(
                    f"{symbol}: UPSERT å®Œæˆ - æˆåŠŸ: "
                    f"{saved_count}, éŒ¯èª¤: {error_count}")

        except Exception as e:
            self.logger.error(f"{symbol}: UPSERT æ“ä½œå¤±æ•—: {e}")
            import traceback
            self.logger.error(
                f"{symbol}: UPSERT è©³ç´°éŒ¯èª¤: {traceback.format_exc()}")
            raise

        return saved_count

    def process_multiple_stocks(
            self, symbols: List[str],
            period: Union[Period, str] = Period.MAX,
            interval: Union[TimeInterval, str] = TimeInterval.DAY_1,
            force_update: bool = False
    ) -> Dict[str, ProcessResult]:
        """æ‰¹é‡è™•ç†å¤šå€‹è‚¡ç¥¨

        Args:
            symbols: è‚¡ç¥¨ä»£è™Ÿåˆ—è¡¨
            period: æ™‚é–“é€±æœŸ
            interval: æ™‚é–“é–“éš”
            force_update: æ˜¯å¦å¼·åˆ¶æ›´æ–°æ‰€æœ‰æ•¸æ“šï¼ˆåŒ…æ‹¬å·²å­˜åœ¨çš„æ•¸æ“šï¼‰
        """
        self.reporter.info(f"é–‹å§‹æ‰¹é‡è™•ç† {len(symbols)} å€‹è‚¡ç¥¨")
        if force_update:
            self.reporter.info("ğŸ”„ å¼·åˆ¶æ›´æ–°æ¨¡å¼å·²å•Ÿç”¨ - å°‡é‡æ–°è™•ç†æ‰€æœ‰æ•¸æ“š")

        results = {}
        success_count = 0
        total_new_records = 0

        for i, symbol in enumerate(symbols, 1):
            print(f"\nğŸ“Š [{i}/{len(symbols)}] è™•ç† {symbol}")

            result = self.process_stock(symbol, period, interval, force_update)
            results[symbol] = result

            if result.success:
                success_count += 1
                total_new_records += result.new_records

                if force_update and result.new_records > 0:
                    print(
                        f"   âœ… å¼·åˆ¶æ›´æ–°æˆåŠŸ | è™•ç†: {result.new_records} ç­† | "
                        f"ç¸½è¨ˆ: {result.total_records} ç­†")
                else:
                    print(
                        f"   âœ… æˆåŠŸ | æ–°å¢: {result.new_records} ç­† | "
                        f"ç¸½è¨ˆ: {result.total_records} ç­†")
                print(f"   ğŸ“… æ™‚é–“ç¯„åœ: {result.date_range}")
                print(f"   â±ï¸  è™•ç†æ™‚é–“: {result.processing_time:.2f} ç§’")
            else:
                print(f"   âŒ å¤±æ•—: {result.error_message}")

        # é¡¯ç¤ºç¸½çµ
        print(f"\n{'='*60}")
        if force_update:
            print("ğŸ“ˆ å¼·åˆ¶æ›´æ–°æ‰¹é‡è™•ç†å®Œæˆ")
        else:
            print("ğŸ“ˆ æ‰¹é‡è™•ç†å®Œæˆ")
        print(f"âœ… æˆåŠŸ: {success_count}/{len(symbols)} å€‹è‚¡ç¥¨")
        print(f"ğŸ“Š ç¸½è™•ç†è¨˜éŒ„: {total_new_records:,} ç­†")
        print(f"{'='*60}")

        return results

    def get_database_statistics(self) -> Dict[str, Any]:
        """ç²å–è³‡æ–™åº«çµ±è¨ˆè³‡è¨Š"""
        try:
            with self.get_connection() as conn:
                # æ•´é«”çµ±è¨ˆ
                overall_query = text("""
                SELECT
                    COUNT(*) as total_records,
                    COUNT(DISTINCT symbol) as unique_symbols,
                    MIN(date) as earliest_date,
                    MAX(date) as latest_date
                FROM stock_data
                """)

                overall_result = conn.execute(overall_query).fetchone()

                # å„è‚¡ç¥¨çµ±è¨ˆ
                symbol_query = text("""
                SELECT
                    symbol,
                    COUNT(*) as record_count,
                    MIN(date) as start_date,
                    MAX(date) as end_date,
                    MAX(updated_at) as last_updated
                FROM stock_data
                GROUP BY symbol
                ORDER BY record_count DESC
                """)

                symbol_results = conn.execute(symbol_query).fetchall()

                return {
                    'total_records': (overall_result[0] if overall_result
                                      else 0),
                    'unique_symbols': (overall_result[1] if overall_result
                                       else 0),
                    'date_range': {
                        'earliest': (overall_result[2] if overall_result
                                     else None),
                        'latest': (overall_result[3] if overall_result
                                   else None)
                    },
                    'symbols': [
                        {
                            'symbol': row[0],
                            'records': row[1],
                            'start_date': row[2],
                            'end_date': row[3],
                            'last_updated': row[4]
                        } for row in symbol_results
                    ]
                }

        except Exception as e:
            self.logger.error(f"ç²å–çµ±è¨ˆè³‡è¨Šå¤±æ•—: {e}")
            return {}


def print_statistics(stats: Dict[str, Any]):
    """åˆ—å°è³‡æ–™åº«çµ±è¨ˆè³‡è¨Š"""
    if not stats:
        print("ç„¡æ³•ç²å–çµ±è¨ˆè³‡è¨Š")
        return

    print("\nğŸ“Š è³‡æ–™åº«çµ±è¨ˆè³‡è¨Š")
    print(f"{'='*50}")
    print(f"ç¸½è¨˜éŒ„æ•¸: {stats['total_records']:,}")
    print(f"è‚¡ç¥¨æ•¸é‡: {stats['unique_symbols']}")

    if stats['date_range']['earliest'] and stats['date_range']['latest']:
        print(
            f"æ—¥æœŸç¯„åœ: {stats['date_range']['earliest']} ~ "
            f"{stats['date_range']['latest']}")

    if stats['symbols']:
        print("\nğŸ“‹ å„è‚¡ç¥¨è©³æƒ…:")
        for symbol_info in stats['symbols'][:10]:  # åªé¡¯ç¤ºå‰10å€‹
            print(f"  {symbol_info['symbol']}: {symbol_info['records']:,} ç­† "
                  f"({symbol_info['start_date']} ~ {symbol_info['end_date']})")

        if len(stats['symbols']) > 10:
            print(f"  ... é‚„æœ‰ {len(stats['symbols']) - 10} å€‹è‚¡ç¥¨")


def main():
    """ä¸»ç¨‹å¼"""
    try:
        # å‰µå»ºåˆ†æå™¨
        analyzer = StockAnalyzerDB()

        # æ¸¬è©¦è³‡æ–™åº«é€£æ¥
        if not analyzer.test_connection():
            print("âŒ è³‡æ–™åº«é€£æ¥å¤±æ•—ï¼Œç¨‹å¼çµæŸ")
            return

        # é è¨­è‚¡ç¥¨åˆ—è¡¨
        default_stocks = ["2330", "AAPL"]

        # å¾å‘½ä»¤è¡Œåƒæ•¸ç²å–è‚¡ç¥¨ä»£è™Ÿ
        if len(sys.argv) > 1:
            target_stocks = sys.argv[1:]
            print(f"â„¹ï¸  ä½¿ç”¨å‘½ä»¤è¡Œåƒæ•¸: {', '.join(target_stocks)}")
        else:
            target_stocks = default_stocks
            print(f"â„¹ï¸  ä½¿ç”¨é è¨­è‚¡ç¥¨: {', '.join(target_stocks)}")

        print("ğŸš€ è‚¡ç¥¨æŠ€è¡“åˆ†æèˆ‡è³‡æ–™åº«æ•´åˆç³»çµ±")
        print(f"ğŸ“… {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")

        # è™•ç†è‚¡ç¥¨
        results = analyzer.process_multiple_stocks(
            symbols=target_stocks,
            period=Period.MAX,
            interval=TimeInterval.DAY_1,
            force_update=False  # å¼·åˆ¶æ›´æ–°æ¨¡å¼
        )

        # é¡¯ç¤ºçµ±è¨ˆè³‡è¨Š
        stats = analyzer.get_database_statistics()
        print_statistics(stats)

        print("\nâœ… ç¨‹å¼åŸ·è¡Œå®Œæˆï¼")
        print("ğŸ“ è©³ç´°æ—¥èªŒè«‹æŸ¥çœ‹: stock_analyzer.log")

        # è¿”å›è™•ç†çµæœä¾›å¾ŒçºŒä½¿ç”¨
        return results

    except Exception as e:
        print(f"\nâŒ ç¨‹å¼åŸ·è¡ŒéŒ¯èª¤: {e}")
        logging.error(f"ä¸»ç¨‹å¼éŒ¯èª¤: {e}", exc_info=True)


if __name__ == "__main__":
    main()
